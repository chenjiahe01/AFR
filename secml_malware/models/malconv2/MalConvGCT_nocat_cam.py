from collections import deque
from collections import OrderedDict 

import random
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
#from torch.utils.checkpoint import checkpoint
import checkpoint #当使用多个GPU时，这个检查点的实现比PyTorch的更快


from LowMemConv import LowMemConvBase
from MalConvML import MalConvML

def getParams():
    # 这个格式是为了让它能以自动化的方式与Optuna轻松合作。
    # 变量名称 -> tuple(sampling function, dict(sampling_args) )
    params = {
        'channels'     : ("suggest_int", {'name':'channels', 'low':32, 'high':1024}),
        'log_stride'   : ("suggest_int", {'name':'log2_stride', 'low':2, 'high':9}),
        'window_size'  : ("suggest_int", {'name':'window_size', 'low':32, 'high':256}),
        'layers'       : ("suggest_int", {'name':'layers', 'low':1, 'high':3}),
        'embd_size'    : ("suggest_int", {'name':'embd_size', 'low':4, 'high':16}),
    }
    return OrderedDict(sorted(params.items(), key=lambda t: t[0]))

def initModel(**kwargs):
    new_args = {}
    for x in getParams():
        if x in kwargs:
            new_args[x] = kwargs[x]
            
    return MalConvGCT(**new_args)


class MalConvGCT(LowMemConvBase):
    
    def __init__(self, out_size=2, channels=128, window_size=512, stride=512, layers=1, embd_size=8, log_stride=None, low_mem=True):
        super(MalConvGCT, self).__init__()
        self.low_mem = low_mem
        self.embd = nn.Embedding(257, embd_size, padding_idx=0)
        if not log_stride is None:
            stride = 2**log_stride
        # 上下文层，与原始MalConv相同
        self.context_net = MalConvML(out_size=channels, channels=channels, window_size=window_size, stride=stride, layers=layers, embd_size=embd_size)

        self.convs = nn.ModuleList([nn.Conv1d(embd_size, channels*2, window_size, stride=stride, bias=True)] + [nn.Conv1d(channels, channels*2, window_size, stride=1, bias=True) for i in range(layers-1)])
        
        #These two objs are not used. They were originally present before the F.glu function existed, and then were accidently left in when we switched over. So the state file provided has unusued states in it. They are left in this definition so that there are no issues loading the file that MalConv was trained on.
        #If you are going to train from scratch, you can delete these two lines.
        #self.convs_1 = nn.ModuleList([nn.Conv1d(channels*2, channels, 1, bias=True) for i in range(layers)])
        #self.convs_atn = nn.ModuleList([nn.Conv1d(channels*2, channels, 1, bias=True) for i in range(layers)])
        
        self.linear_atn = nn.ModuleList([nn.Linear(channels, channels) for i in range(layers)])
        
        #one-by-one cons to perform information sharing
        self.convs_share = nn.ModuleList([nn.Conv1d(channels, channels, 1, bias=True) for i in range(layers)])

        
        self.fc_1 = nn.Linear(channels, channels)
        self.fc_2 = nn.Linear(channels, out_size)
        
    
    #Over-write the determinRF call to use the base context_net to detemrin RF. We should have the same totla RF, and this will simplify logic significantly. 
    def determinRF(self):
        return self.context_net.determinRF()
    
    def processRange(self, x, gct=None):
        """Process a range of data, and return the output."""
        if gct is None:
            raise Exception("No Global Context Given")
        # else:
        #     print("---------------")
        #     print("GCT shape: ", gct.shape)
        x = self.embd(x)
        # x.shape = (1, 16000000, 8)
        x = x.permute(0,2,1)
        # x.shape = (1, 8, 16000000)
        for conv_glu, linear_cntx, conv_share in zip(self.convs, self.linear_atn, self.convs_share):
            x = F.glu(conv_glu(x), dim=1)

            x = F.leaky_relu(conv_share(x))
            x_len = x.shape[2]
            B = x.shape[0]
            C = x.shape[1]
            # print("feature map shape: ", x.shape)
            
            sqrt_dim = np.sqrt(x.shape[1])
            # 我们将需要一个带有时间维度的GCT版本，我们将根据需要调整到合适的长度
            ctnx = torch.tanh(linear_cntx(gct))
            
            # 大小是(B, C)，但我们需要(B, C, 1)来作为1d conv过滤器使用
            ctnx = torch.unsqueeze(ctnx, dim=2)
            # 将批处理的数据滚入通道
            x_tmp = x.view(1,B*C,-1) 
            # 现在我们可以用B组应用一个conv，这样每个批次都能得到自己的上下文，只应用于需要的东西。
            x_tmp = F.conv1d(x_tmp, ctnx, groups=B)
            # x_tmp的形状将是(1, B, L)，现在我们只需要将数据重新排序，回到(B, 1, L)。
            x_gates = x_tmp.view(B, 1, -1)
            
            # 现在我们有效地应用σ(x_t^T tanh(W c))
            gates = torch.sigmoid( x_gates )

            x = x * gates
        #     print("GCG output shape:", x.shape)
        #
        # if gct is not None:
        #     print("---------------")
        return x
    
    def forward(self, x):
        # print("x.shape", x.shape)
        if self.low_mem:
            global_context = checkpoint.CheckpointFunction.apply(self.context_net.seq2fix,1, x)
        else:
            global_context = self.context_net.seq2fix(x)
        # print("global_context.shape", global_context.shape)

        post_conv = x = self.seq2fix(x, pr_args={'gct':global_context})
        # print("post_conv.shape", post_conv.shape)
        penult = x = F.leaky_relu(self.fc_1( x ))
        # print("x.shape", penult.shape)
        x = self.fc_2(x)
        # print("x.shape", x.shape)
        return x, penult, post_conv

    def get_cam(self, x):

        global_context = self.context_net.predict_with_cam(x)
        x = self.processRange(x, gct=global_context)    # gcg output

        # CAM
        feature_list = x[0]
        channels = feature_list.shape[0]        # 输出的通道数
        section_len = feature_list.shape[1]     # 数据切分成的段数
        # print("feature_list.shape", feature_list.shape)
        # fc_1和fc_2的权重矩阵
        weight1 = self.fc_1.weight
        # print("weight1.shape", weight1.shape)
        weight2 = self.fc_2.weight[0]
        # print("weight2.shape", weight2.shape)

        cam_weight = np.array([0] * section_len)
        cam_weight = torch.from_numpy(cam_weight).float()
        for i in range(0, channels):
            for row in weight1:
                cam_weight[i] += row[i] * weight2[i]
        cam = torch.from_numpy(np.array([0] * section_len)).float()
        for i in range(0, channels):
            cam += feature_list[i] * cam_weight[i]

        print("cam", cam)
        return cam



